Pooled OLS
pannel data: 여러 해 모은 데이터, 매년 조사한 데이터 세트

SVM
- takes two support vectors, pick two SV and draw a line that maximizes the two
- margin: 결정 경계와 sv사이의 거리
- scikit-learn으로 모델 구현 가능, 서포트 벡터만 잘 고르면 나머지는 무시 가능 : 속도 빠름
- 하드마진은 오버피팅 가능
- 소프트 마진: 너그러운 마진은 언더피팅 (대충 학습하는 것이라)
pro: good accuracy, efficient because subset of training points used
optimization is very good due to convex nature of optimization function
can use for both linear(hard) and non linear(soft)

cons: works very on clean and small dataset
SVM less efficient on noisy datasets in which classes overlapped with each other
dont really provide probability estimates



ML
- supervised, unsupervised, reinforcement learning
- unsupervised는 tag가 없음, we don't know the true value
- clustering model은 겹치는걸 제대로 맞추는건 어렵

K-means <-> KNN
- 군집 찾기: 1.centroid에 대해서 2. 가까이 있는 centroid에 assign <-> supervised learning: k개의 neighbor를 찾음

Expectation and Maximization (EM 알고리즘)
- E와 M을 번갈아가면서 바복
- rnk를 maximize해줘야, centroid도 optimize




GMM
# https://scikit-learn.org/stable/modules/mixture.html
- 말그대로 Gaussian 분포가 여러개 혼합된 알고리즘
- k 는 직접 설정함
- GMM을 이용해 classification x가 있을 때, k개의 znk를 계산해서 가장 높은 Gaussian distribution을 선택하는 것
- 
